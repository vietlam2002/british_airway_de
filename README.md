# British Airways Extract-Load Project

This project implements a modern Extract-Load pipeline for British Airways, designed to process and analyze customer review data from [Airline Quality](https://www.airlinequality.com/airline-reviews/british-airways/). It leverages **Apache Airflow**, **Snowflake**, **AWS S3**, and **Docker** to load data into Snowflake before transformation using dbt.

---

## ğŸ—‚ Project Structure

```
.
â”œâ”€â”€ airflow/             # Airflow configuration and DAGs
â”‚   â”œâ”€â”€ dags/            # Airflow DAG definitions
â”‚   â”œâ”€â”€ tasks/           # Custom task implementations
â”‚   â”œâ”€â”€ plugins/         # Custom Airflow plugins
â”‚   â””â”€â”€ logs/            # Airflow execution logs
â”œâ”€â”€ data/                # Data files
â”‚   â””â”€â”€ raw_data.csv     # Source data file
â”œâ”€â”€ docker/              # Docker configuration
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ Makefile             # Project automation commands
```

---

## âš™ï¸ Technology Stack
![ba_architecture](https://github.com/user-attachments/assets/d64c1a15-baa5-44a6-a086-49706aff2822)
- **Data Processing**: Python 3.12 with Pandas
- **Workflow Orchestration**: Apache Airflow
- **Data Warehouse**: Snowflake
- **Data Lake**: AWS S3 for staging
- **Containerization**: Docker

---

## ğŸ§± Data Architecture

### 1. Data Source
The project processes customer review data scraped from [AirlineQuality.com](https://www.airlinequality.com/airline-reviews/british-airways/), which contains detailed information about customer flight experiences.

### 2. Data Processing Pipeline
1. **Data Crawling**
   - Crawl customer reviews from AirlineQuality.com
   - Store raw data as `raw_data.csv`

2. **Data Cleaning & Transformation**
   - Process and clean the raw data (see [British Airways Data Cleaning Repository](https://github.com/DucLe-2005/british_airways_data_cleaning))
   - Standardize formats and handle missing values
   - Generate cleaned dataset

3. **Staging in S3**
   - Upload cleaned data to AWS S3 bucket (`upload_cleaned_data_to_s3`)
   - Store in staging area for Snowflake ingestion
   - Maintain data versioning and audit trail

4. **Snowflake Loading**
   - Use Snowflake COPY operator to load data from S3
   - Transform and load into target tables
   - Implement incremental loading strategy

### 3. Data Quality Framework
- Data validation checks
- Error handling and logging
- Pipeline monitoring and alerting
- Snowflake data quality monitoring

---

## ğŸ§© Project Components

### ğŸ“Š Airflow DAGs
Located in `airflow/dags/`:
- DAG definitions for data processing workflows
- Task scheduling and dependency management
- Error handling and retry logic
- Snowflake data loading and transformation tasks

### ğŸ›  Custom Tasks
Located in `airflow/tasks/`:
- Data processing and transformation logic
- S3 upload operations
- Snowflake data loading and unloading operations
- Custom operators for specific business requirements
- Utility functions for data handling

### ğŸ”Œ Airflow Plugins
Located in `airflow/plugins/`:
- Custom hooks and operators
- Extended Airflow functionality
- Integration with Snowflake and S3 services

---

## ğŸ“¦ Key Dependencies

- `pandas==1.5.3`
- `apache-airflow-providers-snowflake`
- `snowflake-connector-python`
- `boto3==1.35.0`
- `apache-airflow-providers-amazon`

---
